{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Fundamentals: Randomized Controlled Trials\n",
    "\n",
    "A/B testing is the practical application of **Randomized Controlled Trials (RCTs)** to decision-making. It's more fundamental than causal inference methods because randomization eliminates the need for complex adjustment strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why randomization is the gold standard for causal inference\n",
    "2. Learn statistical testing with different probability distributions\n",
    "3. Compute sample sizes and power analysis\n",
    "4. Recognize common pitfalls in A/B testing\n",
    "5. Apply A/B testing to biological and business scenarios\n",
    "\n",
    "## Why RCTs Work\n",
    "\n",
    "Randomization achieves four critical goals:\n",
    "\n",
    "1. **Breaks confounder-treatment link**: Treatment assignment is independent of all confounders (measured and unmeasured)\n",
    "2. **Creates comparable groups**: Treated and control groups are statistically identical in expectation\n",
    "3. **Unbiased estimation**: The naive difference-in-means estimator is unbiased\n",
    "4. **No adjustment needed**: We don't need to measure or adjust for confounders\n",
    "\n",
    "This is why A/B testing is so powerful—and why it's the foundation of evidence-based medicine, tech product development, and experimental biology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Magic of Randomization\n",
    "\n",
    "Let's demonstrate why randomization eliminates confounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_observational_vs_rct(n=1000, true_effect=10, seed=42):\n",
    "    \"\"\"\n",
    "    Compare observational study vs RCT for the same treatment.\n",
    "    \n",
    "    Scenario: Job training program\n",
    "    - Confounder: Age (affects both treatment selection and earnings)\n",
    "    - True effect: $10k increase in earnings\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Confounder: age (normalized)\n",
    "    age = np.random.beta(2, 3, n)\n",
    "    \n",
    "    # OBSERVATIONAL: younger people self-select into training\n",
    "    treatment_prob_obs = 1 / (1 + np.exp(5 * (age - 0.3)))\n",
    "    treatment_obs = np.random.binomial(1, treatment_prob_obs)\n",
    "    \n",
    "    # RCT: completely random assignment (50/50)\n",
    "    treatment_rct = np.random.binomial(1, 0.5, n)\n",
    "    \n",
    "    # Outcome: baseline + age effect + treatment effect\n",
    "    baseline = 50\n",
    "    age_effect = 50 * age  # Older → higher earnings\n",
    "    treatment_effect = true_effect\n",
    "    \n",
    "    outcome_obs = baseline + age_effect + treatment_effect * treatment_obs + np.random.normal(0, 5, n)\n",
    "    outcome_rct = baseline + age_effect + treatment_effect * treatment_rct + np.random.normal(0, 5, n)\n",
    "    \n",
    "    # Estimate effects\n",
    "    ate_obs = outcome_obs[treatment_obs == 1].mean() - outcome_obs[treatment_obs == 0].mean()\n",
    "    ate_rct = outcome_rct[treatment_rct == 1].mean() - outcome_rct[treatment_rct == 0].mean()\n",
    "    \n",
    "    # Check balance\n",
    "    age_treated_obs = age[treatment_obs == 1].mean()\n",
    "    age_control_obs = age[treatment_obs == 0].mean()\n",
    "    age_treated_rct = age[treatment_rct == 1].mean()\n",
    "    age_control_rct = age[treatment_rct == 0].mean()\n",
    "    \n",
    "    return {\n",
    "        'true_effect': true_effect,\n",
    "        'ate_obs': ate_obs,\n",
    "        'ate_rct': ate_rct,\n",
    "        'bias_obs': ate_obs - true_effect,\n",
    "        'bias_rct': ate_rct - true_effect,\n",
    "        'age_imbalance_obs': age_treated_obs - age_control_obs,\n",
    "        'age_imbalance_rct': age_treated_rct - age_control_rct,\n",
    "    }\n",
    "\n",
    "results = simulate_observational_vs_rct()\n",
    "\n",
    "print(\"Comparison: Observational Study vs RCT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True effect: ${results['true_effect']:.1f}k\\n\")\n",
    "\n",
    "print(\"OBSERVATIONAL STUDY (biased selection):\")\n",
    "print(f\"  Estimated effect: ${results['ate_obs']:.1f}k\")\n",
    "print(f\"  Bias: ${results['bias_obs']:.1f}k\")\n",
    "print(f\"  Age imbalance: {results['age_imbalance_obs']:.3f}\\n\")\n",
    "\n",
    "print(\"RCT (randomized):\")\n",
    "print(f\"  Estimated effect: ${results['ate_rct']:.1f}k\")\n",
    "print(f\"  Bias: ${results['bias_rct']:.1f}k\")\n",
    "print(f\"  Age imbalance: {results['age_imbalance_rct']:.3f}\")\n",
    "\n",
    "print(\"\\n⭐ Randomization eliminates bias by balancing confounders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Statistical Testing with Different Distributions\n",
    "\n",
    "Different outcome types require different statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Continuous Outcomes: t-test\n",
    "\n",
    "**Use case**: Conversion rate, revenue, gene expression, cell count\n",
    "\n",
    "**Assumptions**: Outcomes are approximately normal (or large sample size via CLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test_continuous(control, treatment, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform two-sample t-test for continuous outcomes.\n",
    "    \"\"\"\n",
    "    # Compute statistics\n",
    "    mean_control = np.mean(control)\n",
    "    mean_treatment = np.mean(treatment)\n",
    "    effect = mean_treatment - mean_control\n",
    "    \n",
    "    # Two-sample t-test\n",
    "    t_stat, p_value = stats.ttest_ind(treatment, control)\n",
    "    \n",
    "    # Confidence interval\n",
    "    se = np.sqrt(np.var(control)/len(control) + np.var(treatment)/len(treatment))\n",
    "    ci_lower = effect - 1.96 * se\n",
    "    ci_upper = effect + 1.96 * se\n",
    "    \n",
    "    return {\n",
    "        'mean_control': mean_control,\n",
    "        'mean_treatment': mean_treatment,\n",
    "        'effect': effect,\n",
    "        'relative_lift': (effect / mean_control) * 100,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'significant': p_value < alpha,\n",
    "    }\n",
    "\n",
    "# Example: Website revenue per user\n",
    "np.random.seed(42)\n",
    "control_revenue = np.random.normal(100, 30, 1000)  # Mean $100, SD $30\n",
    "treatment_revenue = np.random.normal(108, 30, 1000)  # 8% lift\n",
    "\n",
    "result = ab_test_continuous(control_revenue, treatment_revenue)\n",
    "\n",
    "print(\"A/B Test: Continuous Outcome (Revenue)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Control mean: ${result['mean_control']:.2f}\")\n",
    "print(f\"Treatment mean: ${result['mean_treatment']:.2f}\")\n",
    "print(f\"Effect: ${result['effect']:.2f} ({result['relative_lift']:.1f}% lift)\")\n",
    "print(f\"95% CI: [${result['ci_lower']:.2f}, ${result['ci_upper']:.2f}]\")\n",
    "print(f\"p-value: {result['p_value']:.4f}\")\n",
    "print(f\"Significant: {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Binary Outcomes: Proportion Test\n",
    "\n",
    "**Use case**: Click-through rate, conversion, survival, success/failure\n",
    "\n",
    "**Distribution**: Binomial → Normal approximation for large samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test_binary(n_control, successes_control, n_treatment, successes_treatment, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform two-proportion z-test for binary outcomes.\n",
    "    \"\"\"\n",
    "    p_control = successes_control / n_control\n",
    "    p_treatment = successes_treatment / n_treatment\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_pooled = (successes_control + successes_treatment) / (n_control + n_treatment)\n",
    "    \n",
    "    # Standard error\n",
    "    se_pooled = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_control + 1/n_treatment))\n",
    "    \n",
    "    # Z-test\n",
    "    z_stat = (p_treatment - p_control) / se_pooled\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    # Confidence interval (unpooled SE)\n",
    "    se_unpooled = np.sqrt(p_control*(1-p_control)/n_control + p_treatment*(1-p_treatment)/n_treatment)\n",
    "    effect = p_treatment - p_control\n",
    "    ci_lower = effect - 1.96 * se_unpooled\n",
    "    ci_upper = effect + 1.96 * se_unpooled\n",
    "    \n",
    "    return {\n",
    "        'p_control': p_control,\n",
    "        'p_treatment': p_treatment,\n",
    "        'absolute_lift': effect,\n",
    "        'relative_lift': (effect / p_control) * 100,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'significant': p_value < alpha,\n",
    "    }\n",
    "\n",
    "# Example: Click-through rate\n",
    "result = ab_test_binary(\n",
    "    n_control=10000,\n",
    "    successes_control=500,  # 5% CTR\n",
    "    n_treatment=10000,\n",
    "    successes_treatment=575,  # 5.75% CTR\n",
    ")\n",
    "\n",
    "print(\"A/B Test: Binary Outcome (Click-Through Rate)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Control CTR: {result['p_control']*100:.2f}%\")\n",
    "print(f\"Treatment CTR: {result['p_treatment']*100:.2f}%\")\n",
    "print(f\"Absolute lift: {result['absolute_lift']*100:.2f} percentage points\")\n",
    "print(f\"Relative lift: {result['relative_lift']:.1f}%\")\n",
    "print(f\"95% CI: [{result['ci_lower']*100:.2f}%, {result['ci_upper']*100:.2f}%]\")\n",
    "print(f\"p-value: {result['p_value']:.4f}\")\n",
    "print(f\"Significant: {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Count Data: Poisson Test\n",
    "\n",
    "**Use case**: Number of events, page views, mutations, cell divisions\n",
    "\n",
    "**Distribution**: Poisson (for rare events) or Negative Binomial (overdispersed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test_poisson(control_counts, treatment_counts, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compare Poisson rates using likelihood ratio test.\n",
    "    \"\"\"\n",
    "    lambda_control = np.mean(control_counts)\n",
    "    lambda_treatment = np.mean(treatment_counts)\n",
    "    \n",
    "    # For large samples, use normal approximation\n",
    "    se = np.sqrt(lambda_control/len(control_counts) + lambda_treatment/len(treatment_counts))\n",
    "    z_stat = (lambda_treatment - lambda_control) / se\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    effect = lambda_treatment - lambda_control\n",
    "    ci_lower = effect - 1.96 * se\n",
    "    ci_upper = effect + 1.96 * se\n",
    "    \n",
    "    return {\n",
    "        'lambda_control': lambda_control,\n",
    "        'lambda_treatment': lambda_treatment,\n",
    "        'effect': effect,\n",
    "        'relative_lift': (effect / lambda_control) * 100,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'significant': p_value < alpha,\n",
    "    }\n",
    "\n",
    "# Example: Number of mutations per cell\n",
    "np.random.seed(42)\n",
    "control_mutations = np.random.poisson(5, 500)  # Mean 5 mutations\n",
    "treatment_mutations = np.random.poisson(4, 500)  # Treatment reduces to 4\n",
    "\n",
    "result = ab_test_poisson(control_mutations, treatment_mutations)\n",
    "\n",
    "print(\"A/B Test: Count Data (Mutations per Cell)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Control rate: {result['lambda_control']:.2f} mutations/cell\")\n",
    "print(f\"Treatment rate: {result['lambda_treatment']:.2f} mutations/cell\")\n",
    "print(f\"Effect: {result['effect']:.2f} ({result['relative_lift']:.1f}% change)\")\n",
    "print(f\"95% CI: [{result['ci_lower']:.2f}, {result['ci_upper']:.2f}]\")\n",
    "print(f\"p-value: {result['p_value']:.4f}\")\n",
    "print(f\"Significant: {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Time-to-Event: Survival Analysis\n",
    "\n",
    "**Use case**: Customer churn, cell death, time to conversion\n",
    "\n",
    "**Distribution**: Exponential, Weibull\n",
    "\n",
    "**Test**: Log-rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Time to customer churn (exponential)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Control: mean survival time = 12 months\n",
    "control_survival = np.random.exponential(12, 200)\n",
    "\n",
    "# Treatment: mean survival time = 15 months (25% improvement)\n",
    "treatment_survival = np.random.exponential(15, 200)\n",
    "\n",
    "# Simple comparison of means\n",
    "mean_control = np.mean(control_survival)\n",
    "mean_treatment = np.mean(treatment_survival)\n",
    "effect = mean_treatment - mean_control\n",
    "\n",
    "# t-test (valid for exponential with large samples)\n",
    "t_stat, p_value = stats.ttest_ind(treatment_survival, control_survival)\n",
    "\n",
    "print(\"A/B Test: Time-to-Event (Customer Churn)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Control mean survival: {mean_control:.1f} months\")\n",
    "print(f\"Treatment mean survival: {mean_treatment:.1f} months\")\n",
    "print(f\"Effect: {effect:.1f} months ({(effect/mean_control)*100:.1f}% improvement)\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Visualize survival curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "time_points = np.linspace(0, 50, 100)\n",
    "survival_control = np.exp(-time_points / mean_control)\n",
    "survival_treatment = np.exp(-time_points / mean_treatment)\n",
    "\n",
    "ax.plot(time_points, survival_control, label='Control', linewidth=2)\n",
    "ax.plot(time_points, survival_treatment, label='Treatment', linewidth=2)\n",
    "ax.set_xlabel('Time (months)')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_title('Survival Curves: Treatment Extends Customer Lifetime')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sample Size and Power Analysis\n",
    "\n",
    "Before running an A/B test, we need to determine:\n",
    "- **How many samples** do we need?\n",
    "- **What effect size** can we detect?\n",
    "- **What is our statistical power**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size_continuous(baseline_mean, mde, baseline_std, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for continuous outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    baseline_mean : float\n",
    "        Mean of control group\n",
    "    mde : float\n",
    "        Minimum detectable effect (absolute)\n",
    "    baseline_std : float\n",
    "        Standard deviation\n",
    "    alpha : float\n",
    "        Significance level (Type I error rate)\n",
    "    power : float\n",
    "        Statistical power (1 - Type II error rate)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int : Required sample size per group\n",
    "    \"\"\"\n",
    "    # Effect size (Cohen's d)\n",
    "    effect_size = mde / baseline_std\n",
    "    \n",
    "    # Z-scores\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    # Sample size formula\n",
    "    n = 2 * ((z_alpha + z_beta) / effect_size)**2\n",
    "    \n",
    "    return int(np.ceil(n))\n",
    "\n",
    "# Example: Revenue per user\n",
    "n_required = sample_size_continuous(\n",
    "    baseline_mean=100,\n",
    "    mde=5,  # Want to detect $5 difference\n",
    "    baseline_std=30,\n",
    "    alpha=0.05,\n",
    "    power=0.80\n",
    ")\n",
    "\n",
    "print(\"Sample Size Calculation\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline mean: $100\")\n",
    "print(f\"Minimum detectable effect: $5 (5% lift)\")\n",
    "print(f\"Standard deviation: $30\")\n",
    "print(f\"Significance level: 0.05\")\n",
    "print(f\"Power: 0.80\")\n",
    "print(f\"\\nRequired sample size per group: {n_required:,}\")\n",
    "print(f\"Total sample size: {2*n_required:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size_binary(p_control, mde_relative, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for binary outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_control : float\n",
    "        Baseline conversion rate\n",
    "    mde_relative : float\n",
    "        Minimum detectable effect (relative, e.g., 0.10 for 10% lift)\n",
    "    alpha : float\n",
    "        Significance level\n",
    "    power : float\n",
    "        Statistical power\n",
    "    \"\"\"\n",
    "    p_treatment = p_control * (1 + mde_relative)\n",
    "    \n",
    "    # Z-scores\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_pooled = (p_control + p_treatment) / 2\n",
    "    \n",
    "    # Sample size formula\n",
    "    numerator = (z_alpha * np.sqrt(2 * p_pooled * (1 - p_pooled)) + \n",
    "                 z_beta * np.sqrt(p_control * (1 - p_control) + p_treatment * (1 - p_treatment)))**2\n",
    "    denominator = (p_treatment - p_control)**2\n",
    "    \n",
    "    n = numerator / denominator\n",
    "    \n",
    "    return int(np.ceil(n))\n",
    "\n",
    "# Example: Conversion rate\n",
    "n_required = sample_size_binary(\n",
    "    p_control=0.05,  # 5% baseline\n",
    "    mde_relative=0.10,  # Want to detect 10% relative lift (5% → 5.5%)\n",
    "    alpha=0.05,\n",
    "    power=0.80\n",
    ")\n",
    "\n",
    "print(\"Sample Size Calculation (Binary)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline conversion rate: 5.0%\")\n",
    "print(f\"Minimum detectable effect: 10% relative lift (5.0% → 5.5%)\")\n",
    "print(f\"Significance level: 0.05\")\n",
    "print(f\"Power: 0.80\")\n",
    "print(f\"\\nRequired sample size per group: {n_required:,}\")\n",
    "print(f\"Total sample size: {2*n_required:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Curves\n",
    "\n",
    "Visualize the relationship between sample size, effect size, and power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power as a function of sample size\n",
    "sample_sizes = np.arange(100, 5000, 100)\n",
    "effect_sizes = [0.1, 0.2, 0.3]  # Small, medium, large\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for es in effect_sizes:\n",
    "    powers = []\n",
    "    for n in sample_sizes:\n",
    "        # Compute power\n",
    "        z_alpha = stats.norm.ppf(0.975)\n",
    "        ncp = es * np.sqrt(n / 2)  # Non-centrality parameter\n",
    "        power = 1 - stats.norm.cdf(z_alpha - ncp)\n",
    "        powers.append(power)\n",
    "    \n",
    "    ax.plot(sample_sizes, powers, label=f'Effect size = {es}', linewidth=2)\n",
    "\n",
    "ax.axhline(0.80, color='red', linestyle='--', label='80% power target')\n",
    "ax.set_xlabel('Sample Size per Group')\n",
    "ax.set_ylabel('Statistical Power')\n",
    "ax.set_title('Power Curves for Different Effect Sizes')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Smaller effects require larger samples to detect reliably.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Common Pitfalls\n",
    "\n",
    "### 4.1 Peeking (Sequential Testing)\n",
    "\n",
    "Checking results multiple times inflates Type I error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_peeking_problem(n_peeks=10, n_final=1000, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Demonstrate how peeking inflates false positive rate.\n",
    "    \"\"\"\n",
    "    false_positives_no_peek = 0\n",
    "    false_positives_peek = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate data with NO true effect\n",
    "        control = np.random.normal(0, 1, n_final)\n",
    "        treatment = np.random.normal(0, 1, n_final)  # Same distribution!\n",
    "        \n",
    "        # No peeking: test only at the end\n",
    "        _, p_final = stats.ttest_ind(treatment, control)\n",
    "        if p_final < 0.05:\n",
    "            false_positives_no_peek += 1\n",
    "        \n",
    "        # Peeking: test at multiple points\n",
    "        peek_points = np.linspace(100, n_final, n_peeks, dtype=int)\n",
    "        stopped_early = False\n",
    "        for n in peek_points:\n",
    "            _, p = stats.ttest_ind(treatment[:n], control[:n])\n",
    "            if p < 0.05:\n",
    "                false_positives_peek += 1\n",
    "                stopped_early = True\n",
    "                break\n",
    "    \n",
    "    fpr_no_peek = false_positives_no_peek / n_simulations\n",
    "    fpr_peek = false_positives_peek / n_simulations\n",
    "    \n",
    "    return fpr_no_peek, fpr_peek\n",
    "\n",
    "fpr_no_peek, fpr_peek = simulate_peeking_problem()\n",
    "\n",
    "print(\"The Peeking Problem\")\n",
    "print(\"=\"*50)\n",
    "print(f\"False positive rate (no peeking): {fpr_no_peek:.3f} (should be ~0.05)\")\n",
    "print(f\"False positive rate (peeking 10x): {fpr_peek:.3f}\")\n",
    "print(f\"\\n⚠️  Peeking inflates false positives by {fpr_peek/fpr_no_peek:.1f}x!\")\n",
    "print(\"\\nSolution: Use sequential testing methods (e.g., alpha spending functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multiple Testing\n",
    "\n",
    "Testing many metrics simultaneously increases false discovery rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate testing 20 metrics with no true effects\n",
    "np.random.seed(42)\n",
    "n_metrics = 20\n",
    "n_per_group = 1000\n",
    "\n",
    "p_values = []\n",
    "for i in range(n_metrics):\n",
    "    control = np.random.normal(0, 1, n_per_group)\n",
    "    treatment = np.random.normal(0, 1, n_per_group)\n",
    "    _, p = stats.ttest_ind(treatment, control)\n",
    "    p_values.append(p)\n",
    "\n",
    "# Uncorrected\n",
    "n_significant_uncorrected = sum(p < 0.05 for p in p_values)\n",
    "\n",
    "# Bonferroni correction\n",
    "alpha_bonferroni = 0.05 / n_metrics\n",
    "n_significant_bonferroni = sum(p < alpha_bonferroni for p in p_values)\n",
    "\n",
    "print(\"Multiple Testing Problem\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of metrics tested: {n_metrics}\")\n",
    "print(f\"True effects: 0 (all null)\")\n",
    "print(f\"\\nSignificant at α=0.05 (uncorrected): {n_significant_uncorrected}\")\n",
    "print(f\"Expected false positives: {n_metrics * 0.05:.1f}\")\n",
    "print(f\"\\nSignificant with Bonferroni correction: {n_significant_bonferroni}\")\n",
    "print(f\"\\n⚠️  Without correction, we'd falsely declare {n_significant_uncorrected} 'winners'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Bayesian A/B Testing\n",
    "\n",
    "An alternative to frequentist testing using probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_ab_test_binary(n_control, successes_control, n_treatment, successes_treatment,\n",
    "                              prior_alpha=1, prior_beta=1):\n",
    "    \"\"\"\n",
    "    Bayesian A/B test for binary outcomes using Beta-Binomial model.\n",
    "    \n",
    "    Prior: Beta(alpha, beta)\n",
    "    Posterior: Beta(alpha + successes, beta + failures)\n",
    "    \"\"\"\n",
    "    # Posterior distributions\n",
    "    posterior_control = stats.beta(\n",
    "        prior_alpha + successes_control,\n",
    "        prior_beta + (n_control - successes_control)\n",
    "    )\n",
    "    \n",
    "    posterior_treatment = stats.beta(\n",
    "        prior_alpha + successes_treatment,\n",
    "        prior_beta + (n_treatment - successes_treatment)\n",
    "    )\n",
    "    \n",
    "    # Sample from posteriors\n",
    "    n_samples = 100000\n",
    "    samples_control = posterior_control.rvs(n_samples)\n",
    "    samples_treatment = posterior_treatment.rvs(n_samples)\n",
    "    \n",
    "    # Probability that treatment > control\n",
    "    prob_treatment_better = np.mean(samples_treatment > samples_control)\n",
    "    \n",
    "    # Expected lift\n",
    "    lift_samples = (samples_treatment - samples_control) / samples_control\n",
    "    expected_lift = np.mean(lift_samples)\n",
    "    lift_ci = np.percentile(lift_samples, [2.5, 97.5])\n",
    "    \n",
    "    return {\n",
    "        'prob_treatment_better': prob_treatment_better,\n",
    "        'expected_lift': expected_lift,\n",
    "        'lift_ci_lower': lift_ci[0],\n",
    "        'lift_ci_upper': lift_ci[1],\n",
    "        'posterior_control': posterior_control,\n",
    "        'posterior_treatment': posterior_treatment,\n",
    "    }\n",
    "\n",
    "# Example\n",
    "result = bayesian_ab_test_binary(\n",
    "    n_control=1000,\n",
    "    successes_control=50,\n",
    "    n_treatment=1000,\n",
    "    successes_treatment=65\n",
    ")\n",
    "\n",
    "print(\"Bayesian A/B Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Probability treatment is better: {result['prob_treatment_better']:.1%}\")\n",
    "print(f\"Expected relative lift: {result['expected_lift']*100:.1f}%\")\n",
    "print(f\"95% Credible interval: [{result['lift_ci_lower']*100:.1f}%, {result['lift_ci_upper']*100:.1f}%]\")\n",
    "\n",
    "# Visualize posteriors\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.linspace(0, 0.15, 1000)\n",
    "ax.plot(x, result['posterior_control'].pdf(x), label='Control', linewidth=2)\n",
    "ax.plot(x, result['posterior_treatment'].pdf(x), label='Treatment', linewidth=2)\n",
    "ax.set_xlabel('Conversion Rate')\n",
    "ax.set_ylabel('Probability Density')\n",
    "ax.set_title('Posterior Distributions of Conversion Rates')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Randomization is magic**: It eliminates confounding without needing to measure or adjust for confounders\n",
    "2. **Choose the right test**: Match your statistical test to your outcome distribution\n",
    "3. **Plan sample size**: Don't run underpowered experiments\n",
    "4. **Avoid peeking**: Sequential testing requires special methods\n",
    "5. **Correct for multiple testing**: Testing many metrics inflates false positives\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Outcome Type | Distribution | Test |\n",
    "|--------------|--------------|------|\n",
    "| Continuous (revenue, expression) | Normal | t-test |\n",
    "| Binary (conversion, click) | Binomial | Proportion test |\n",
    "| Count (events, mutations) | Poisson | Poisson test |\n",
    "| Time-to-event (churn, survival) | Exponential/Weibull | Log-rank test |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- `../causal_inference/01_treatment_effects.ipynb`: What to do when randomization isn't possible\n",
    "- `../causal_inference/02_causal_graphs.ipynb`: Understanding confounding with DAGs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
