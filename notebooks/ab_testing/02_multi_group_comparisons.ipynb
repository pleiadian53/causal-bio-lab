{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Group Comparisons: Beyond A/B Testing\n",
    "\n",
    "A/B testing compares two groups. But what if we have **multiple treatments** or **multiple classifiers** to compare?\n",
    "\n",
    "This notebook covers statistical methods for comparing 3+ groups, with special focus on:\n",
    "- **Friedman test**: Non-parametric test for repeated measures (e.g., multiple classifiers on same datasets)\n",
    "- **Nemenyi post-hoc test**: Pairwise comparisons after Friedman\n",
    "- **Kruskal-Wallis**: Non-parametric test for independent groups\n",
    "- **ANOVA**: Parametric alternative\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand when to use each multi-group test\n",
    "2. Apply Friedman test to ensemble learning scenarios\n",
    "3. Perform post-hoc pairwise comparisons correctly\n",
    "4. Visualize results with critical difference diagrams\n",
    "5. Avoid multiple testing pitfalls\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "**Use cases:**\n",
    "- Comparing multiple ML models on the same datasets (your ensemble learning case)\n",
    "- Testing multiple drug treatments\n",
    "- Comparing gene expression across multiple conditions\n",
    "- A/B/C/D/... testing in product development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare, kruskal, f_oneway\n",
    "from itertools import combinations\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Problem with Multiple Pairwise Tests\n",
    "\n",
    "Why can't we just do multiple A/B tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multiple testing problem\n",
    "def multiple_pairwise_problem(k_groups=5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Show how pairwise testing inflates Type I error.\n",
    "    \n",
    "    k_groups: Number of groups to compare\n",
    "    \"\"\"\n",
    "    # Number of pairwise comparisons\n",
    "    n_comparisons = k_groups * (k_groups - 1) // 2\n",
    "    \n",
    "    # Family-wise error rate (FWER)\n",
    "    # Probability of at least one false positive\n",
    "    fwer = 1 - (1 - alpha)**n_comparisons\n",
    "    \n",
    "    return n_comparisons, fwer\n",
    "\n",
    "print(\"Multiple Testing Problem\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Groups':<10} {'Comparisons':<15} {'FWER (α=0.05)':<20}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for k in [2, 3, 4, 5, 10]:\n",
    "    n_comp, fwer = multiple_pairwise_problem(k)\n",
    "    print(f\"{k:<10} {n_comp:<15} {fwer:.3f}\")\n",
    "\n",
    "print(\"\\n⚠️  With 5 groups, we have 40% chance of false positive!\")\n",
    "print(\"Solution: Use omnibus test first, then post-hoc corrections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Friedman Test (Repeated Measures)\n",
    "\n",
    "**When to use**: Comparing k treatments/algorithms on the **same** subjects/datasets\n",
    "\n",
    "**Example**: Comparing multiple classifiers on the same benchmark datasets\n",
    "\n",
    "### The Setup\n",
    "\n",
    "- **Rows**: Datasets (or subjects, blocks)\n",
    "- **Columns**: Classifiers (or treatments)\n",
    "- **Values**: Performance metric (accuracy, F1, etc.)\n",
    "\n",
    "Friedman test ranks each row, then tests if mean ranks differ across columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example data: 5 classifiers on 10 datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "n_datasets = 10\n",
    "classifiers = ['Random Forest', 'XGBoost', 'SVM', 'Logistic Reg', 'Neural Net']\n",
    "\n",
    "# Simulate accuracies (RF and XGBoost are better)\n",
    "data = {\n",
    "    'Random Forest': np.random.beta(8, 2, n_datasets),\n",
    "    'XGBoost': np.random.beta(8, 2, n_datasets),\n",
    "    'SVM': np.random.beta(6, 3, n_datasets),\n",
    "    'Logistic Reg': np.random.beta(5, 4, n_datasets),\n",
    "    'Neural Net': np.random.beta(7, 3, n_datasets),\n",
    "}\n",
    "\n",
    "df_classifiers = pd.DataFrame(data)\n",
    "df_classifiers.index = [f'Dataset_{i+1}' for i in range(n_datasets)]\n",
    "\n",
    "print(\"Classifier Performance (Accuracy)\")\n",
    "print(\"=\"*70)\n",
    "print(df_classifiers.round(3))\n",
    "print(f\"\\nMean accuracy per classifier:\")\n",
    "print(df_classifiers.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Friedman test\n",
    "statistic, p_value = friedmanchisquare(*[df_classifiers[col] for col in df_classifiers.columns])\n",
    "\n",
    "print(\"Friedman Test Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test statistic (χ²): {statistic:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05: {p_value < 0.05}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\n✓ Reject null hypothesis: Classifiers have different performance\")\n",
    "    print(\"  → Proceed to post-hoc tests to find which pairs differ\")\n",
    "else:\n",
    "    print(\"\\n✗ Fail to reject null: No evidence of differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Friedman Test\n",
    "\n",
    "**How it works:**\n",
    "1. Rank each dataset (row) from 1 to k\n",
    "2. Compute mean rank for each classifier\n",
    "3. Test if mean ranks differ significantly\n",
    "\n",
    "**Null hypothesis**: All classifiers have the same distribution\n",
    "\n",
    "**Advantages**:\n",
    "- Non-parametric (no normality assumption)\n",
    "- Accounts for dataset-specific difficulty\n",
    "- More powerful than independent tests when data is paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize ranks\n",
    "ranks = df_classifiers.rank(axis=1, ascending=False)\n",
    "mean_ranks = ranks.mean()\n",
    "\n",
    "print(\"Mean Ranks (lower is better)\")\n",
    "print(\"=\"*50)\n",
    "print(mean_ranks.sort_values().round(2))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mean accuracy\n",
    "df_classifiers.mean().sort_values(ascending=False).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Mean Accuracy')\n",
    "axes[0].set_title('Mean Performance Across Datasets')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean ranks\n",
    "mean_ranks.sort_values().plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_xlabel('Mean Rank (lower is better)')\n",
    "axes[1].set_title('Friedman Test Rankings')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Nemenyi Post-Hoc Test\n",
    "\n",
    "After Friedman test rejects the null, we need **post-hoc tests** to determine which pairs differ.\n",
    "\n",
    "**Nemenyi test**: All pairwise comparisons with family-wise error rate control\n",
    "\n",
    "### Critical Difference\n",
    "\n",
    "Two classifiers are significantly different if their mean rank difference exceeds the **critical difference (CD)**:\n",
    "\n",
    "$$CD = q_\\alpha \\sqrt{\\frac{k(k+1)}{6N}}$$\n",
    "\n",
    "where:\n",
    "- $q_\\alpha$ is the critical value from Studentized range distribution\n",
    "- $k$ is the number of classifiers\n",
    "- $N$ is the number of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nemenyi_test(ranks_df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Nemenyi post-hoc test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ranks_df : DataFrame\n",
    "        Ranks for each dataset (rows) and classifier (columns)\n",
    "    alpha : float\n",
    "        Significance level\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with critical difference and pairwise comparisons\n",
    "    \"\"\"\n",
    "    k = ranks_df.shape[1]  # Number of classifiers\n",
    "    N = ranks_df.shape[0]  # Number of datasets\n",
    "    \n",
    "    # Critical value from Studentized range distribution\n",
    "    # For Nemenyi, we use q_alpha / sqrt(2)\n",
    "    from scipy.stats import studentized_range\n",
    "    q_alpha = studentized_range.ppf(1 - alpha, k, np.inf) / np.sqrt(2)\n",
    "    \n",
    "    # Critical difference\n",
    "    cd = q_alpha * np.sqrt(k * (k + 1) / (6 * N))\n",
    "    \n",
    "    # Mean ranks\n",
    "    mean_ranks = ranks_df.mean()\n",
    "    \n",
    "    # Pairwise comparisons\n",
    "    comparisons = []\n",
    "    for clf1, clf2 in combinations(ranks_df.columns, 2):\n",
    "        rank_diff = abs(mean_ranks[clf1] - mean_ranks[clf2])\n",
    "        significant = rank_diff > cd\n",
    "        comparisons.append({\n",
    "            'classifier_1': clf1,\n",
    "            'classifier_2': clf2,\n",
    "            'rank_diff': rank_diff,\n",
    "            'significant': significant,\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'critical_difference': cd,\n",
    "        'mean_ranks': mean_ranks,\n",
    "        'comparisons': pd.DataFrame(comparisons),\n",
    "    }\n",
    "\n",
    "# Apply Nemenyi test\n",
    "nemenyi_results = nemenyi_test(ranks)\n",
    "\n",
    "print(\"Nemenyi Post-Hoc Test\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Critical Difference (α=0.05): {nemenyi_results['critical_difference']:.3f}\")\n",
    "print(f\"\\nMean Ranks:\")\n",
    "print(nemenyi_results['mean_ranks'].sort_values().round(3))\n",
    "print(f\"\\nPairwise Comparisons:\")\n",
    "print(nemenyi_results['comparisons'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Difference Diagram\n",
    "\n",
    "A standard visualization in ML benchmarking literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_critical_difference(mean_ranks, cd, figsize=(12, 4)):\n",
    "    \"\"\"\n",
    "    Plot critical difference diagram.\n",
    "    \n",
    "    Classifiers connected by a horizontal line are NOT significantly different.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Sort by rank\n",
    "    sorted_ranks = mean_ranks.sort_values()\n",
    "    n_classifiers = len(sorted_ranks)\n",
    "    \n",
    "    # Plot ranks on x-axis\n",
    "    y_positions = np.arange(n_classifiers)\n",
    "    \n",
    "    # Draw classifier names and ranks\n",
    "    for i, (clf, rank) in enumerate(sorted_ranks.items()):\n",
    "        ax.plot(rank, i, 'o', markersize=10, color='steelblue')\n",
    "        ax.text(rank, i + 0.3, clf, ha='center', fontsize=10, fontweight='bold')\n",
    "        ax.text(rank, i - 0.3, f'{rank:.2f}', ha='center', fontsize=9, color='gray')\n",
    "    \n",
    "    # Draw critical difference bars\n",
    "    # Connect classifiers that are NOT significantly different\n",
    "    clf_list = sorted_ranks.index.tolist()\n",
    "    for i in range(n_classifiers):\n",
    "        for j in range(i + 1, n_classifiers):\n",
    "            rank_diff = abs(sorted_ranks.iloc[j] - sorted_ranks.iloc[i])\n",
    "            if rank_diff <= cd:\n",
    "                # Not significant - draw connecting line\n",
    "                y_mid = (i + j) / 2\n",
    "                ax.plot([sorted_ranks.iloc[i], sorted_ranks.iloc[j]], \n",
    "                       [y_mid, y_mid], 'k-', linewidth=2, alpha=0.3)\n",
    "    \n",
    "    # Add CD reference\n",
    "    ax.plot([1, 1 + cd], [-0.5, -0.5], 'r-', linewidth=3, label=f'CD = {cd:.2f}')\n",
    "    ax.text(1 + cd/2, -0.7, f'Critical Difference', ha='center', color='red', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Mean Rank (lower is better)', fontsize=12)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim(-1, n_classifiers)\n",
    "    ax.set_title('Critical Difference Diagram (Nemenyi Test)\\nClassifiers connected by lines are not significantly different', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_critical_difference(nemenyi_results['mean_ranks'], nemenyi_results['critical_difference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Kruskal-Wallis Test (Independent Groups)\n",
    "\n",
    "**When to use**: Comparing k groups that are **independent** (not repeated measures)\n",
    "\n",
    "**Example**: Comparing gene expression across multiple tissue types (different samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate independent group data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Gene expression in 4 different tissues\n",
    "tissue_a = np.random.lognormal(3, 0.5, 30)\n",
    "tissue_b = np.random.lognormal(3.2, 0.5, 30)\n",
    "tissue_c = np.random.lognormal(3.5, 0.5, 30)\n",
    "tissue_d = np.random.lognormal(3.1, 0.5, 30)\n",
    "\n",
    "# Kruskal-Wallis test\n",
    "statistic, p_value = kruskal(tissue_a, tissue_b, tissue_c, tissue_d)\n",
    "\n",
    "print(\"Kruskal-Wallis Test: Gene Expression Across Tissues\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test statistic (H): {statistic:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05: {p_value < 0.05}\")\n",
    "\n",
    "# Visualize\n",
    "data_tissues = pd.DataFrame({\n",
    "    'Tissue A': tissue_a,\n",
    "    'Tissue B': tissue_b,\n",
    "    'Tissue C': tissue_c,\n",
    "    'Tissue D': tissue_d,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "data_tissues.boxplot(ax=ax)\n",
    "ax.set_ylabel('Gene Expression (log scale)')\n",
    "ax.set_title('Gene Expression Across Tissue Types')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dunn's Post-Hoc Test\n",
    "\n",
    "After Kruskal-Wallis, use **Dunn's test** for pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_test(groups, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Dunn's post-hoc test with Bonferroni correction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    groups : dict\n",
    "        Dictionary of group_name: data arrays\n",
    "    alpha : float\n",
    "        Significance level\n",
    "    \"\"\"\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    \n",
    "    group_names = list(groups.keys())\n",
    "    n_comparisons = len(group_names) * (len(group_names) - 1) // 2\n",
    "    alpha_corrected = alpha / n_comparisons  # Bonferroni\n",
    "    \n",
    "    results = []\n",
    "    for g1, g2 in combinations(group_names, 2):\n",
    "        stat, p = mannwhitneyu(groups[g1], groups[g2], alternative='two-sided')\n",
    "        results.append({\n",
    "            'group_1': g1,\n",
    "            'group_2': g2,\n",
    "            'statistic': stat,\n",
    "            'p_value': p,\n",
    "            'p_corrected': p * n_comparisons,  # Bonferroni adjustment\n",
    "            'significant': p < alpha_corrected,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "groups_dict = {\n",
    "    'Tissue A': tissue_a,\n",
    "    'Tissue B': tissue_b,\n",
    "    'Tissue C': tissue_c,\n",
    "    'Tissue D': tissue_d,\n",
    "}\n",
    "\n",
    "dunn_results = dunn_test(groups_dict)\n",
    "\n",
    "print(\"Dunn's Post-Hoc Test (Bonferroni corrected)\")\n",
    "print(\"=\"*70)\n",
    "print(dunn_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: ANOVA (Parametric Alternative)\n",
    "\n",
    "**When to use**: Data is approximately normal, equal variances\n",
    "\n",
    "**Advantage**: More powerful than non-parametric tests when assumptions hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal data\n",
    "np.random.seed(42)\n",
    "\n",
    "group1 = np.random.normal(100, 15, 50)\n",
    "group2 = np.random.normal(105, 15, 50)\n",
    "group3 = np.random.normal(110, 15, 50)\n",
    "group4 = np.random.normal(103, 15, 50)\n",
    "\n",
    "# One-way ANOVA\n",
    "f_stat, p_value = f_oneway(group1, group2, group3, group4)\n",
    "\n",
    "print(\"One-Way ANOVA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"F-statistic: {f_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05: {p_value < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tukey's HSD Post-Hoc\n",
    "\n",
    "After ANOVA, use **Tukey's Honestly Significant Difference** test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import tukey_hsd\n",
    "\n",
    "# Perform Tukey HSD\n",
    "res = tukey_hsd(group1, group2, group3, group4)\n",
    "\n",
    "print(\"Tukey's HSD Post-Hoc Test\")\n",
    "print(\"=\"*50)\n",
    "print(\"Pairwise confidence intervals:\")\n",
    "print(res.confidence_interval())\n",
    "print(f\"\\nPairwise p-values:\")\n",
    "print(res.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Practical Application - Ensemble Learning\n",
    "\n",
    "Complete workflow for comparing multiple classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifiers_workflow(performance_df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Complete workflow for comparing multiple classifiers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    performance_df : DataFrame\n",
    "        Rows = datasets, Columns = classifiers, Values = performance metric\n",
    "    alpha : float\n",
    "        Significance level\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CLASSIFIER COMPARISON WORKFLOW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Descriptive statistics\n",
    "    print(\"\\n1. DESCRIPTIVE STATISTICS\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"Mean performance:\")\n",
    "    print(performance_df.mean().sort_values(ascending=False).round(4))\n",
    "    \n",
    "    # Step 2: Friedman test\n",
    "    print(\"\\n2. FRIEDMAN TEST (Omnibus)\")\n",
    "    print(\"-\"*70)\n",
    "    stat, p = friedmanchisquare(*[performance_df[col] for col in performance_df.columns])\n",
    "    print(f\"χ² = {stat:.3f}, p = {p:.4f}\")\n",
    "    \n",
    "    if p >= alpha:\n",
    "        print(f\"\\n✗ No significant differences detected (p ≥ {alpha})\")\n",
    "        print(\"  → Stop here. No need for post-hoc tests.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n✓ Significant differences detected (p < {alpha})\")\n",
    "    print(\"  → Proceed to post-hoc tests\")\n",
    "    \n",
    "    # Step 3: Compute ranks\n",
    "    ranks = performance_df.rank(axis=1, ascending=False)\n",
    "    mean_ranks = ranks.mean().sort_values()\n",
    "    \n",
    "    print(\"\\n3. MEAN RANKS\")\n",
    "    print(\"-\"*70)\n",
    "    print(mean_ranks.round(3))\n",
    "    \n",
    "    # Step 4: Nemenyi test\n",
    "    print(\"\\n4. NEMENYI POST-HOC TEST\")\n",
    "    print(\"-\"*70)\n",
    "    nemenyi_res = nemenyi_test(ranks, alpha)\n",
    "    print(f\"Critical Difference: {nemenyi_res['critical_difference']:.3f}\")\n",
    "    print(\"\\nSignificant pairwise differences:\")\n",
    "    sig_comparisons = nemenyi_res['comparisons'][nemenyi_res['comparisons']['significant']]\n",
    "    if len(sig_comparisons) > 0:\n",
    "        print(sig_comparisons[['classifier_1', 'classifier_2', 'rank_diff']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"  (none)\")\n",
    "    \n",
    "    # Step 5: Visualization\n",
    "    print(\"\\n5. CRITICAL DIFFERENCE DIAGRAM\")\n",
    "    print(\"-\"*70)\n",
    "    plot_critical_difference(mean_ranks, nemenyi_res['critical_difference'])\n",
    "    \n",
    "    return nemenyi_res\n",
    "\n",
    "# Apply to our classifier data\n",
    "results = compare_classifiers_workflow(df_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Decision Guide\n",
    "\n",
    "### Which Test Should I Use?\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────┐\n",
    "│ How many groups?                    │\n",
    "└─────────────────────────────────────┘\n",
    "           │\n",
    "           ├─ 2 groups → Use A/B testing (t-test, Mann-Whitney)\n",
    "           │\n",
    "           └─ 3+ groups\n",
    "                  │\n",
    "                  ├─ Repeated measures? (same subjects/datasets)\n",
    "                  │     │\n",
    "                  │     ├─ Yes → FRIEDMAN TEST\n",
    "                  │     │         Post-hoc: NEMENYI\n",
    "                  │     │\n",
    "                  │     └─ No → Independent groups\n",
    "                  │               │\n",
    "                  │               ├─ Normal + equal variance?\n",
    "                  │               │     │\n",
    "                  │               │     ├─ Yes → ANOVA\n",
    "                  │               │     │         Post-hoc: TUKEY HSD\n",
    "                  │               │     │\n",
    "                  │               │     └─ No → KRUSKAL-WALLIS\n",
    "                  │               │               Post-hoc: DUNN\n",
    "```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Test | Data Type | Groups | Assumptions | Post-Hoc |\n",
    "|------|-----------|--------|-------------|----------|\n",
    "| **Friedman** | Repeated measures | 3+ | Non-parametric | Nemenyi |\n",
    "| **Kruskal-Wallis** | Independent | 3+ | Non-parametric | Dunn |\n",
    "| **ANOVA** | Independent | 3+ | Normal, equal variance | Tukey HSD |\n",
    "| **t-test** | Independent | 2 | Normal | N/A |\n",
    "| **Mann-Whitney** | Independent | 2 | Non-parametric | N/A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Don't do multiple pairwise tests**: Use omnibus test first to control family-wise error rate\n",
    "2. **Friedman test is ideal for ML benchmarking**: Accounts for dataset-specific difficulty\n",
    "3. **Nemenyi test controls FWER**: Safe for all pairwise comparisons after Friedman\n",
    "4. **Critical difference diagrams**: Standard visualization in ML literature\n",
    "5. **Choose the right test**: Match your test to your data structure (repeated vs independent)\n",
    "\n",
    "### Connection to A/B Testing\n",
    "\n",
    "- **A/B testing**: Special case with k=2 groups\n",
    "- **Multi-group testing**: Generalization to k≥3 groups\n",
    "- **Same principles**: Randomization, hypothesis testing, multiple testing correction\n",
    "\n",
    "### When to Use This\n",
    "\n",
    "**Your ensemble learning case**: Comparing multiple constituent classifiers\n",
    "- Use **Friedman test** (repeated measures on same datasets)\n",
    "- Follow with **Nemenyi post-hoc**\n",
    "- Visualize with **critical difference diagram**\n",
    "\n",
    "### References\n",
    "\n",
    "- Demšar (2006). \"Statistical Comparisons of Classifiers over Multiple Data Sets\"\n",
    "- García & Herrera (2008). \"An Extension on Statistical Comparisons of Classifiers\"\n",
    "- Benavoli et al. (2016). \"Should We Really Use Post-Hoc Tests?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
