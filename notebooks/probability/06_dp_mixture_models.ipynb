{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Process Mixture Models\n",
    "\n",
    "**DP Mixture Models (DPMMs)** combine the Dirichlet Process with parametric mixture components for flexible, nonparametric clustering.\n",
    "\n",
    "## Why This Matters for Causal ML\n",
    "\n",
    "1. **Heterogeneous Treatment Effects** — Discover patient subgroups with different CATEs\n",
    "2. **Flexible Outcome Modeling** — Nonparametric priors for potential outcomes\n",
    "3. **Clustering for Stratification** — Data-driven patient stratification\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [DPMM Overview](#1-dpmm-overview)\n",
    "2. [Gaussian DPMM Implementation](#2-gaussian-dpmm-implementation)\n",
    "3. [Application: Treatment Effect Heterogeneity](#3-application-treatment-effect-heterogeneity)\n",
    "4. [Using sklearn](#4-using-sklearn)\n",
    "5. [Quick Reference](#5-quick-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DPMM Overview\n",
    "\n",
    "### The Model\n",
    "\n",
    "$$G \\sim \\text{DP}(\\alpha, G_0)$$\n",
    "$$\\theta_i | G \\sim G$$\n",
    "$$x_i | \\theta_i \\sim F(\\theta_i)$$\n",
    "\n",
    "Where:\n",
    "- $G_0$ is the base measure (prior over component parameters)\n",
    "- $F(\\theta)$ is the likelihood (e.g., Gaussian)\n",
    "- Multiple observations share the same $\\theta$ → clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mixture data\n",
    "true_means = [-3, 0, 2, 5]\n",
    "true_stds = [0.5, 0.8, 0.4, 0.6]\n",
    "true_weights = [0.3, 0.25, 0.25, 0.2]\n",
    "\n",
    "n_samples = 300\n",
    "true_assignments = np.random.choice(4, n_samples, p=true_weights)\n",
    "data = np.array([np.random.randn() * true_stds[k] + true_means[k] \n",
    "                 for k in true_assignments])\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(data, bins=40, density=True, alpha=0.5, edgecolor='black')\n",
    "\n",
    "x = np.linspace(-6, 8, 200)\n",
    "for k, (mu, std, w) in enumerate(zip(true_means, true_stds, true_weights)):\n",
    "    ax.plot(x, w * stats.norm.pdf(x, mu, std), '--', lw=2, label=f'Component {k}')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('True Mixture (K=4)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian DPMM Implementation\n",
    "\n",
    "A simple collapsed Gibbs sampler for 1D Gaussian DPMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDPMM:\n",
    "    \"\"\"Simple Gaussian DPMM with collapsed Gibbs sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, mu0=0.0, sigma0=3.0, sigma=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.mu0 = mu0\n",
    "        self.sigma0 = sigma0\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def _posterior_params(self, data_k):\n",
    "        n = len(data_k)\n",
    "        if n == 0:\n",
    "            return self.mu0, self.sigma0\n",
    "        precision0 = 1 / self.sigma0**2\n",
    "        precision_data = n / self.sigma**2\n",
    "        precision_post = precision0 + precision_data\n",
    "        sigma_post = np.sqrt(1 / precision_post)\n",
    "        mu_post = (precision0 * self.mu0 + precision_data * np.mean(data_k)) / precision_post\n",
    "        return mu_post, sigma_post\n",
    "    \n",
    "    def _marginal_likelihood(self, x, data_k):\n",
    "        mu_post, sigma_post = self._posterior_params(data_k)\n",
    "        pred_var = sigma_post**2 + self.sigma**2\n",
    "        return stats.norm.pdf(x, mu_post, np.sqrt(pred_var))\n",
    "    \n",
    "    def fit(self, data, n_iter=100):\n",
    "        n = len(data)\n",
    "        assignments = np.arange(n)\n",
    "        history = []\n",
    "        \n",
    "        for iteration in range(n_iter):\n",
    "            for i in np.random.permutation(n):\n",
    "                temp_assignments = assignments.copy()\n",
    "                temp_assignments[i] = -1\n",
    "                \n",
    "                log_probs, cluster_list = [], []\n",
    "                for k in set(temp_assignments) - {-1}:\n",
    "                    mask = temp_assignments == k\n",
    "                    n_k = np.sum(mask)\n",
    "                    log_prob = np.log(n_k) + np.log(self._marginal_likelihood(data[i], data[mask]))\n",
    "                    log_probs.append(log_prob)\n",
    "                    cluster_list.append(k)\n",
    "                \n",
    "                log_prob_new = np.log(self.alpha) + np.log(self._marginal_likelihood(data[i], []))\n",
    "                log_probs.append(log_prob_new)\n",
    "                cluster_list.append(max(cluster_list) + 1 if cluster_list else 0)\n",
    "                \n",
    "                log_probs = np.array(log_probs)\n",
    "                probs = np.exp(log_probs - logsumexp(log_probs))\n",
    "                assignments[i] = np.random.choice(cluster_list, p=probs)\n",
    "            \n",
    "            # Relabel\n",
    "            unique = sorted(set(assignments))\n",
    "            mapping = {old: new for new, old in enumerate(unique)}\n",
    "            assignments = np.array([mapping[k] for k in assignments])\n",
    "            history.append(len(set(assignments)))\n",
    "            \n",
    "            if (iteration + 1) % 25 == 0:\n",
    "                print(f\"Iter {iteration+1}: {len(set(assignments))} clusters\")\n",
    "        \n",
    "        self.assignments_ = assignments\n",
    "        self.history_ = history\n",
    "        return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DPMM\n",
    "model = GaussianDPMM(alpha=2.0, mu0=0.0, sigma0=5.0, sigma=0.6)\n",
    "assignments = model.fit(data, n_iter=100)\n",
    "\n",
    "n_clusters = len(set(assignments))\n",
    "print(f\"\\nFound {n_clusters} clusters (true: 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Cluster count over iterations\n",
    "axes[0].plot(model.history_, 'b-', lw=2)\n",
    "axes[0].axhline(4, color='red', linestyle='--', label='True K=4')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Number of clusters')\n",
    "axes[0].set_title('Convergence')\n",
    "axes[0].legend()\n",
    "\n",
    "# Inferred clusters\n",
    "colors = plt.cm.tab10(np.arange(n_clusters))\n",
    "for k in range(n_clusters):\n",
    "    mask = assignments == k\n",
    "    axes[1].scatter(np.arange(len(data))[mask], data[mask], c=[colors[k]], alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('x')\n",
    "axes[1].set_title(f'Inferred Clusters (K={n_clusters})')\n",
    "\n",
    "# Compare means\n",
    "inferred_means = sorted([data[assignments == k].mean() for k in range(n_clusters)])\n",
    "axes[2].bar(np.arange(4) - 0.2, sorted(true_means), width=0.4, alpha=0.7, label='True')\n",
    "axes[2].bar(np.arange(len(inferred_means)) + 0.2, inferred_means, width=0.4, alpha=0.7, label='Inferred')\n",
    "axes[2].set_xlabel('Cluster')\n",
    "axes[2].set_ylabel('Mean')\n",
    "axes[2].set_title('Cluster Means')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application: Treatment Effect Heterogeneity\n",
    "\n",
    "Discovering patient subgroups with different treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate heterogeneous treatment effects\n",
    "np.random.seed(42)\n",
    "n_patients = 400\n",
    "\n",
    "# True subgroups\n",
    "subgroup_effects = {0: 0.0, 1: 3.0, 2: 8.0, 3: -2.0}\n",
    "subgroup_probs = [0.35, 0.30, 0.20, 0.15]\n",
    "\n",
    "true_subgroups = np.random.choice(4, n_patients, p=subgroup_probs)\n",
    "true_effects = np.array([subgroup_effects[s] for s in true_subgroups])\n",
    "\n",
    "# Treatment and outcomes\n",
    "treatment = np.random.binomial(1, 0.5, n_patients)\n",
    "baseline = np.random.randn(n_patients) * 2 + 10\n",
    "noise = np.random.randn(n_patients) * 1.5\n",
    "\n",
    "Y0 = baseline + noise\n",
    "Y1 = baseline + true_effects + noise\n",
    "Y_obs = treatment * Y1 + (1 - treatment) * Y0\n",
    "\n",
    "naive_ate = Y_obs[treatment == 1].mean() - Y_obs[treatment == 0].mean()\n",
    "true_ate = true_effects.mean()\n",
    "\n",
    "print(f\"True ATE: {true_ate:.2f}\")\n",
    "print(f\"Naive ATE: {naive_ate:.2f}\")\n",
    "print(f\"True subgroup effects: {list(subgroup_effects.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize heterogeneity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Effect distribution\n",
    "axes[0].hist(true_effects, bins=20, density=True, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(true_ate, color='red', linestyle='--', lw=2, label=f'ATE = {true_ate:.2f}')\n",
    "axes[0].set_xlabel('Treatment Effect')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('True Effect Distribution (Heterogeneous)')\n",
    "axes[0].legend()\n",
    "\n",
    "# By subgroup\n",
    "colors = plt.cm.tab10(np.arange(4))\n",
    "for s in range(4):\n",
    "    mask = true_subgroups == s\n",
    "    axes[1].scatter(baseline[mask], true_effects[mask], c=[colors[s]], \n",
    "                    alpha=0.5, s=30, label=f'Subgroup {s} (τ={subgroup_effects[s]})')\n",
    "axes[1].set_xlabel('Baseline')\n",
    "axes[1].set_ylabel('Treatment Effect')\n",
    "axes[1].set_title('Effects by Subgroup')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DPMM to discover subgroups\n",
    "model_hte = GaussianDPMM(alpha=2.0, mu0=0.0, sigma0=5.0, sigma=0.5)\n",
    "inferred = model_hte.fit(true_effects, n_iter=100)\n",
    "\n",
    "n_found = len(set(inferred))\n",
    "print(f\"\\nFound {n_found} subgroups\")\n",
    "\n",
    "for k in range(n_found):\n",
    "    mask = inferred == k\n",
    "    print(f\"  Subgroup {k}: n={np.sum(mask)}, effect={true_effects[mask].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using sklearn\n",
    "\n",
    "For practical use, sklearn provides `BayesianGaussianMixture` with a DP-like prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# Fit with sklearn\n",
    "bgm = BayesianGaussianMixture(\n",
    "    n_components=10,  # Upper bound\n",
    "    weight_concentration_prior_type='dirichlet_process',\n",
    "    weight_concentration_prior=2.0,  # Concentration\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bgm.fit(data.reshape(-1, 1))\n",
    "sklearn_assignments = bgm.predict(data.reshape(-1, 1))\n",
    "\n",
    "# Count active components\n",
    "weights = bgm.weights_\n",
    "active = np.sum(weights > 0.01)\n",
    "print(f\"sklearn found {active} active components\")\n",
    "print(f\"Weights: {weights[weights > 0.01].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our implementation vs sklearn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Our implementation\n",
    "for k in range(n_clusters):\n",
    "    mask = assignments == k\n",
    "    axes[0].scatter(np.arange(len(data))[mask], data[mask], alpha=0.6, s=20)\n",
    "axes[0].set_title(f'Our DPMM: {n_clusters} clusters')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('x')\n",
    "\n",
    "# sklearn\n",
    "for k in np.unique(sklearn_assignments):\n",
    "    mask = sklearn_assignments == k\n",
    "    axes[1].scatter(np.arange(len(data))[mask], data[mask], alpha=0.6, s=20)\n",
    "axes[1].set_title(f'sklearn BGM: {len(np.unique(sklearn_assignments))} clusters')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Reference\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **DPMM** | DP prior + parametric likelihood |\n",
    "| **Concentration α** | Controls expected number of clusters |\n",
    "| **Collapsed Gibbs** | Integrate out component parameters |\n",
    "| **sklearn BGM** | Practical implementation with DP prior |\n",
    "\n",
    "### When to Use DPMM\n",
    "\n",
    "- Unknown number of clusters\n",
    "- Heterogeneous treatment effects\n",
    "- Flexible density estimation\n",
    "- Patient stratification\n",
    "\n",
    "### sklearn Usage\n",
    "\n",
    "```python\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "model = BayesianGaussianMixture(\n",
    "    n_components=20,  # Upper bound\n",
    "    weight_concentration_prior_type='dirichlet_process',\n",
    "    weight_concentration_prior=alpha\n",
    ")\n",
    "model.fit(X)\n",
    "labels = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "DPMMs are powerful for:\n",
    "\n",
    "1. **Nonparametric clustering** — data determines K\n",
    "2. **Treatment effect heterogeneity** — discover patient subgroups\n",
    "3. **Flexible modeling** — complex outcome distributions\n",
    "\n",
    "**For causal ML:** Use DPMMs to discover latent subgroups with different treatment effects, enabling personalized treatment decisions.\n",
    "\n",
    "**Next steps:** Return to core causal ML topics (ATE, CATE, causal graphs) in the main project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
