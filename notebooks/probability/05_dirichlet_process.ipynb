{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Process: Infinite-Dimensional Dirichlet\n",
    "\n",
    "The **Dirichlet Process (DP)** is a distribution over probability distributions. It extends the finite Dirichlet to infinite dimensions, allowing the data to determine the number of clusters.\n",
    "\n",
    "## Why This Matters for Causal ML\n",
    "\n",
    "1. **Heterogeneous treatment effects** — Discover patient subgroups with different CATEs without pre-specifying K\n",
    "2. **Flexible outcome modeling** — Nonparametric priors for potential outcomes $Y(0)$, $Y(1)$\n",
    "3. **Latent confounders** — Model unknown confounding structure\n",
    "4. **Clustering for stratification** — Data-driven patient stratification\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [From Dirichlet to Dirichlet Process](#1-from-dirichlet-to-dirichlet-process)\n",
    "2. [Stick-Breaking Construction](#2-stick-breaking-construction)\n",
    "3. [Chinese Restaurant Process](#3-chinese-restaurant-process)\n",
    "4. [The Concentration Parameter](#4-the-concentration-parameter)\n",
    "5. [Posterior Inference](#5-posterior-inference)\n",
    "6. [Connection to Causal ML](#6-connection-to-causal-ml)\n",
    "7. [Quick Reference](#7-quick-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Dirichlet to Dirichlet Process\n",
    "\n",
    "### The Limitation of Finite Dirichlet\n",
    "\n",
    "With Dirichlet, we must specify K categories upfront:\n",
    "\n",
    "$$\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_K) \\sim \\text{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)$$\n",
    "\n",
    "**Problem:** What if we don't know K? What if K should grow with data?\n",
    "\n",
    "### The Dirichlet Process Solution\n",
    "\n",
    "The DP is a distribution over **probability measures**:\n",
    "\n",
    "$$G \\sim \\text{DP}(\\alpha, G_0)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha > 0$ is the **concentration parameter**\n",
    "- $G_0$ is the **base measure** (prior guess for G)\n",
    "- $G$ is a random probability measure (discrete with probability 1)\n",
    "\n",
    "### Key Property: Discreteness\n",
    "\n",
    "Even if $G_0$ is continuous, samples from a DP are **discrete** (atomic). This is what enables clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate: DP draws are discrete even with continuous base measure\n",
    "np.random.seed(42)\n",
    "\n",
    "def sample_dp_naive(alpha, G0_sampler, n_samples):\n",
    "    \"\"\"\n",
    "    Naive DP sampling via stick-breaking (for illustration).\n",
    "    Returns atoms and their weights.\n",
    "    \"\"\"\n",
    "    # Stick-breaking weights\n",
    "    K = 100  # Truncation\n",
    "    betas = np.random.beta(1, alpha, K)\n",
    "    weights = np.zeros(K)\n",
    "    remaining = 1.0\n",
    "    for k in range(K):\n",
    "        weights[k] = betas[k] * remaining\n",
    "        remaining *= (1 - betas[k])\n",
    "    \n",
    "    # Atoms from base measure\n",
    "    atoms = G0_sampler(K)\n",
    "    \n",
    "    # Sample from the discrete distribution\n",
    "    indices = np.random.choice(K, size=n_samples, p=weights/weights.sum())\n",
    "    samples = atoms[indices]\n",
    "    \n",
    "    return samples, atoms, weights\n",
    "\n",
    "# Base measure: standard normal\n",
    "G0_sampler = lambda n: np.random.randn(n)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, alpha in zip(axes, [0.5, 2, 10]):\n",
    "    samples, atoms, weights = sample_dp_naive(alpha, G0_sampler, 1000)\n",
    "    \n",
    "    # Plot histogram of samples\n",
    "    ax.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Overlay base measure\n",
    "    x = np.linspace(-4, 4, 200)\n",
    "    ax.plot(x, stats.norm.pdf(x), 'r--', lw=2, label='Base measure $G_0$')\n",
    "    \n",
    "    # Count unique values (clusters)\n",
    "    n_unique = len(np.unique(np.round(samples, 6)))\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'DP(α={alpha}, $G_0$=Normal)\\n{n_unique} unique atoms in 1000 samples')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('DP Samples are Discrete (Clustered)', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stick-Breaking Construction\n",
    "\n",
    "The most intuitive way to understand DP is through **stick-breaking** (Sethuraman, 1994).\n",
    "\n",
    "### The Construction\n",
    "\n",
    "Imagine a stick of length 1. We break it sequentially:\n",
    "\n",
    "1. Break off fraction $V_1 \\sim \\text{Beta}(1, \\alpha)$ → weight $\\pi_1 = V_1$\n",
    "2. From remaining $(1-V_1)$, break off $V_2 \\sim \\text{Beta}(1, \\alpha)$ → weight $\\pi_2 = V_2(1-V_1)$\n",
    "3. Continue forever...\n",
    "\n",
    "$$\\pi_k = V_k \\prod_{j=1}^{k-1}(1 - V_j), \\quad V_k \\stackrel{iid}{\\sim} \\text{Beta}(1, \\alpha)$$\n",
    "\n",
    "Each weight $\\pi_k$ is associated with an atom $\\theta_k \\sim G_0$:\n",
    "\n",
    "$$G = \\sum_{k=1}^{\\infty} \\pi_k \\delta_{\\theta_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_breaking(alpha, K=50):\n",
    "    \"\"\"\n",
    "    Generate stick-breaking weights for DP.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Concentration parameter\n",
    "        K: Truncation level\n",
    "        \n",
    "    Returns:\n",
    "        weights: Array of K weights summing to ~1\n",
    "    \"\"\"\n",
    "    betas = np.random.beta(1, alpha, K)\n",
    "    weights = np.zeros(K)\n",
    "    remaining = 1.0\n",
    "    \n",
    "    for k in range(K):\n",
    "        weights[k] = betas[k] * remaining\n",
    "        remaining *= (1 - betas[k])\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Visualize stick-breaking process\n",
    "np.random.seed(42)\n",
    "alpha = 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: The stick-breaking process visually\n",
    "ax = axes[0, 0]\n",
    "K = 10\n",
    "betas = np.random.beta(1, alpha, K)\n",
    "weights = []\n",
    "remaining = 1.0\n",
    "\n",
    "colors = plt.cm.tab10(np.arange(K))\n",
    "left = 0\n",
    "for k in range(K):\n",
    "    w = betas[k] * remaining\n",
    "    weights.append(w)\n",
    "    ax.barh(0, w, left=left, height=0.5, color=colors[k], edgecolor='black')\n",
    "    if w > 0.03:\n",
    "        ax.text(left + w/2, 0, f'π{k+1}', ha='center', va='center', fontsize=10)\n",
    "    left += w\n",
    "    remaining *= (1 - betas[k])\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "ax.set_xlabel('Cumulative weight')\n",
    "ax.set_title(f'Stick-Breaking Visualization (α={alpha})')\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Top right: Weight distribution\n",
    "ax = axes[0, 1]\n",
    "ax.bar(range(1, K+1), weights, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Component k')\n",
    "ax.set_ylabel('Weight πₖ')\n",
    "ax.set_title(f'Stick-Breaking Weights (sum = {sum(weights):.4f})')\n",
    "\n",
    "# Bottom: Multiple draws for different α\n",
    "for ax, alpha in zip(axes[1], [0.5, 10]):\n",
    "    for i in range(5):\n",
    "        weights = stick_breaking(alpha, K=30)\n",
    "        ax.plot(range(1, 31), weights, 'o-', alpha=0.5, markersize=4)\n",
    "    \n",
    "    ax.set_xlabel('Component k')\n",
    "    ax.set_ylabel('Weight πₖ')\n",
    "    ax.set_title(f'Multiple Draws: α = {alpha}')\n",
    "    ax.set_xlim(0, 31)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  - Small α → first few components dominate (few clusters)\")\n",
    "print(\"  - Large α → weights spread across many components (many clusters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected number of components with weight > threshold\n",
    "np.random.seed(42)\n",
    "\n",
    "alphas = [0.1, 0.5, 1, 2, 5, 10, 20]\n",
    "thresholds = [0.01, 0.05, 0.1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for thresh in thresholds:\n",
    "    n_components = []\n",
    "    for alpha in alphas:\n",
    "        counts = []\n",
    "        for _ in range(100):\n",
    "            weights = stick_breaking(alpha, K=100)\n",
    "            counts.append(np.sum(weights > thresh))\n",
    "        n_components.append(np.mean(counts))\n",
    "    \n",
    "    ax.plot(alphas, n_components, 'o-', lw=2, markersize=8, \n",
    "            label=f'Weight > {thresh}')\n",
    "\n",
    "ax.set_xlabel('Concentration α')\n",
    "ax.set_ylabel('Expected number of components')\n",
    "ax.set_title('Number of \"Active\" Components vs Concentration')\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chinese Restaurant Process\n",
    "\n",
    "The **Chinese Restaurant Process (CRP)** is an equivalent way to understand DP, focused on clustering.\n",
    "\n",
    "### The Metaphor\n",
    "\n",
    "Imagine customers entering a restaurant with infinite tables:\n",
    "\n",
    "1. **Customer 1** sits at table 1\n",
    "2. **Customer n+1** either:\n",
    "   - Joins existing table k with probability $\\frac{n_k}{n + \\alpha}$ (where $n_k$ = customers at table k)\n",
    "   - Starts a new table with probability $\\frac{\\alpha}{n + \\alpha}$\n",
    "\n",
    "### The \"Rich Get Richer\" Property\n",
    "\n",
    "Popular tables attract more customers → **preferential attachment**\n",
    "\n",
    "This creates power-law-like cluster size distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_restaurant_process(n_customers, alpha):\n",
    "    \"\"\"\n",
    "    Simulate the Chinese Restaurant Process.\n",
    "    \n",
    "    Args:\n",
    "        n_customers: Number of customers\n",
    "        alpha: Concentration parameter\n",
    "        \n",
    "    Returns:\n",
    "        assignments: Table assignment for each customer\n",
    "        table_counts: Number of customers at each table\n",
    "    \"\"\"\n",
    "    assignments = []\n",
    "    table_counts = []  # Number at each table\n",
    "    \n",
    "    for n in range(n_customers):\n",
    "        if n == 0:\n",
    "            # First customer starts table 1\n",
    "            assignments.append(0)\n",
    "            table_counts.append(1)\n",
    "        else:\n",
    "            # Compute probabilities\n",
    "            probs = np.array(table_counts + [alpha]) / (n + alpha)\n",
    "            \n",
    "            # Sample table\n",
    "            table = np.random.choice(len(probs), p=probs)\n",
    "            \n",
    "            if table == len(table_counts):\n",
    "                # New table\n",
    "                assignments.append(len(table_counts))\n",
    "                table_counts.append(1)\n",
    "            else:\n",
    "                # Existing table\n",
    "                assignments.append(table)\n",
    "                table_counts[table] += 1\n",
    "    \n",
    "    return np.array(assignments), np.array(table_counts)\n",
    "\n",
    "# Visualize CRP\n",
    "np.random.seed(42)\n",
    "n_customers = 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax_row, alpha in zip(axes, [1, 10]):\n",
    "    assignments, table_counts = chinese_restaurant_process(n_customers, alpha)\n",
    "    n_tables = len(table_counts)\n",
    "    \n",
    "    # Seating arrangement\n",
    "    ax_row[0].scatter(range(n_customers), assignments, c=assignments, cmap='tab20', s=30)\n",
    "    ax_row[0].set_xlabel('Customer arrival order')\n",
    "    ax_row[0].set_ylabel('Table assignment')\n",
    "    ax_row[0].set_title(f'CRP(α={alpha}): Seating over time\\n{n_tables} tables')\n",
    "    \n",
    "    # Table sizes\n",
    "    sorted_counts = np.sort(table_counts)[::-1]\n",
    "    ax_row[1].bar(range(1, n_tables+1), sorted_counts, color='steelblue', edgecolor='black')\n",
    "    ax_row[1].set_xlabel('Table (sorted by size)')\n",
    "    ax_row[1].set_ylabel('Number of customers')\n",
    "    ax_row[1].set_title(f'Table Size Distribution')\n",
    "    \n",
    "    # Multiple runs: number of tables\n",
    "    n_tables_runs = []\n",
    "    for _ in range(500):\n",
    "        _, counts = chinese_restaurant_process(n_customers, alpha)\n",
    "        n_tables_runs.append(len(counts))\n",
    "    \n",
    "    ax_row[2].hist(n_tables_runs, bins=20, density=True, alpha=0.7, edgecolor='black')\n",
    "    ax_row[2].axvline(np.mean(n_tables_runs), color='red', linestyle='--', \n",
    "                      label=f'Mean = {np.mean(n_tables_runs):.1f}')\n",
    "    ax_row[2].set_xlabel('Number of tables')\n",
    "    ax_row[2].set_ylabel('Density')\n",
    "    ax_row[2].set_title(f'Distribution of #Tables (500 runs)')\n",
    "    ax_row[2].legend()\n",
    "\n",
    "plt.suptitle('Chinese Restaurant Process', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected number of tables: E[K] ≈ α log(1 + n/α)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_values = [10, 50, 100, 500, 1000]\n",
    "alphas = [0.5, 1, 2, 5, 10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for alpha in alphas:\n",
    "    empirical = []\n",
    "    theoretical = []\n",
    "    \n",
    "    for n in n_values:\n",
    "        # Empirical\n",
    "        n_tables = [len(chinese_restaurant_process(n, alpha)[1]) for _ in range(100)]\n",
    "        empirical.append(np.mean(n_tables))\n",
    "        \n",
    "        # Theoretical approximation\n",
    "        theoretical.append(alpha * np.log(1 + n/alpha))\n",
    "    \n",
    "    ax.plot(n_values, empirical, 'o-', lw=2, label=f'α={alpha} (empirical)')\n",
    "    ax.plot(n_values, theoretical, '--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Number of customers (n)')\n",
    "ax.set_ylabel('Expected number of tables')\n",
    "ax.set_title('Expected Number of Clusters: E[K] ≈ α log(1 + n/α)')\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Key result: Number of clusters grows logarithmically with n\")\n",
    "print(\"This is much slower than linear — DP is parsimonious!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Concentration Parameter\n",
    "\n",
    "The concentration parameter $\\alpha$ is the key hyperparameter:\n",
    "\n",
    "| α | Effect |\n",
    "|---|--------|\n",
    "| Small (< 1) | Few large clusters, strong \"rich get richer\" |\n",
    "| α = 1 | Moderate clustering |\n",
    "| Large (> 10) | Many small clusters, closer to base measure |\n",
    "| α → ∞ | Approaches iid samples from $G_0$ |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- $\\alpha$ = \"pseudo-count\" for a new cluster\n",
    "- Probability of new cluster = $\\frac{\\alpha}{n + \\alpha}$\n",
    "- Expected number of clusters ≈ $\\alpha \\log(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of α effect\n",
    "np.random.seed(42)\n",
    "n_customers = 200\n",
    "alphas = [0.1, 0.5, 1, 2, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for ax, alpha in zip(axes.flat, alphas):\n",
    "    assignments, table_counts = chinese_restaurant_process(n_customers, alpha)\n",
    "    n_tables = len(table_counts)\n",
    "    \n",
    "    # Plot cluster sizes (sorted)\n",
    "    sorted_counts = np.sort(table_counts)[::-1]\n",
    "    ax.bar(range(1, min(21, n_tables+1)), sorted_counts[:20], \n",
    "           color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Cluster rank')\n",
    "    ax.set_ylabel('Size')\n",
    "    ax.set_title(f'α = {alpha}\\n{n_tables} clusters')\n",
    "    ax.set_xlim(0, 21)\n",
    "\n",
    "plt.suptitle(f'Effect of Concentration on Clustering (n={n_customers})', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Posterior Inference\n",
    "\n",
    "Given observations, the DP posterior is also a DP (conjugacy!):\n",
    "\n",
    "$$G | x_1, \\ldots, x_n \\sim \\text{DP}\\left(\\alpha + n, \\frac{\\alpha G_0 + \\sum_{i=1}^n \\delta_{x_i}}{\\alpha + n}\\right)$$\n",
    "\n",
    "The posterior base measure is a mixture of:\n",
    "- Prior base measure $G_0$ (weight $\\alpha$)\n",
    "- Empirical distribution of data (weight $n$)\n",
    "\n",
    "### Predictive Distribution\n",
    "\n",
    "For a new observation $x_{n+1}$:\n",
    "\n",
    "$$x_{n+1} | x_1, \\ldots, x_n \\sim \\frac{\\alpha}{\\alpha + n} G_0 + \\frac{1}{\\alpha + n} \\sum_{i=1}^n \\delta_{x_i}$$\n",
    "\n",
    "This is the **Pólya urn** scheme: either draw from $G_0$ or copy an existing observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pólya urn / Blackwell-MacQueen scheme\n",
    "def polya_urn(alpha, G0_sampler, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from DP using Pólya urn scheme.\n",
    "    \n",
    "    Each new sample either:\n",
    "    - Comes from G0 (prob α/(α+n))\n",
    "    - Copies an existing sample (prob n/(α+n))\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for n in range(n_samples):\n",
    "        if n == 0 or np.random.rand() < alpha / (alpha + n):\n",
    "            # Draw from base measure\n",
    "            samples.append(G0_sampler())\n",
    "        else:\n",
    "            # Copy existing sample\n",
    "            samples.append(np.random.choice(samples))\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# Visualize Pólya urn\n",
    "np.random.seed(42)\n",
    "G0_sampler = lambda: np.random.randn()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, alpha in zip(axes, [0.5, 2, 10]):\n",
    "    samples = polya_urn(alpha, G0_sampler, 500)\n",
    "    \n",
    "    ax.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Base measure\n",
    "    x = np.linspace(-4, 4, 200)\n",
    "    ax.plot(x, stats.norm.pdf(x), 'r--', lw=2, label='$G_0$ = Normal(0,1)')\n",
    "    \n",
    "    n_unique = len(np.unique(np.round(samples, 10)))\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Pólya Urn (α={alpha})\\n{n_unique} unique values')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Pólya Urn Sampling from DP', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connection to Causal ML\n",
    "\n",
    "### Why DP Matters for Causal Inference\n",
    "\n",
    "1. **Heterogeneous Treatment Effects (HTE)**\n",
    "   - Patients may belong to unknown subgroups with different treatment effects\n",
    "   - DP mixture models can discover these subgroups\n",
    "   - No need to pre-specify the number of subgroups\n",
    "\n",
    "2. **Flexible Outcome Modeling**\n",
    "   - Potential outcomes $Y(0)$, $Y(1)$ may have complex distributions\n",
    "   - DP mixtures provide flexible nonparametric priors\n",
    "\n",
    "3. **Instrumental Variables**\n",
    "   - Latent compliance types (compliers, always-takers, never-takers)\n",
    "   - DP can model unknown compliance structure\n",
    "\n",
    "4. **Confounding**\n",
    "   - Latent confounders may cluster observations\n",
    "   - DP can discover confounding structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Discovering treatment effect heterogeneity\n",
    "np.random.seed(42)\n",
    "\n",
    "# True subgroups (unknown to us)\n",
    "n_patients = 300\n",
    "true_subgroups = np.random.choice([0, 1, 2], n_patients, p=[0.5, 0.3, 0.2])\n",
    "\n",
    "# True treatment effects by subgroup\n",
    "true_effects = {0: 0, 1: 5, 2: -3}  # Non-responders, responders, adverse\n",
    "\n",
    "# Generate data\n",
    "treatment = np.random.binomial(1, 0.5, n_patients)\n",
    "baseline = np.random.randn(n_patients) * 2\n",
    "noise = np.random.randn(n_patients) * 1\n",
    "\n",
    "# Outcome depends on subgroup\n",
    "individual_effects = np.array([true_effects[s] for s in true_subgroups])\n",
    "outcome = baseline + treatment * individual_effects + noise\n",
    "\n",
    "# Naive ATE estimate\n",
    "naive_ate = outcome[treatment == 1].mean() - outcome[treatment == 0].mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# True subgroups\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "for s in range(3):\n",
    "    mask = true_subgroups == s\n",
    "    axes[0].scatter(baseline[mask], outcome[mask], c=colors[s], \n",
    "                    alpha=0.5, label=f'Subgroup {s} (effect={true_effects[s]})')\n",
    "axes[0].set_xlabel('Baseline')\n",
    "axes[0].set_ylabel('Outcome')\n",
    "axes[0].set_title('True Subgroups (Unknown)')\n",
    "axes[0].legend()\n",
    "\n",
    "# What we observe\n",
    "axes[1].scatter(baseline[treatment==0], outcome[treatment==0], \n",
    "                alpha=0.5, label='Control', c='steelblue')\n",
    "axes[1].scatter(baseline[treatment==1], outcome[treatment==1], \n",
    "                alpha=0.5, label='Treated', c='coral')\n",
    "axes[1].set_xlabel('Baseline')\n",
    "axes[1].set_ylabel('Outcome')\n",
    "axes[1].set_title(f'Observed Data\\nNaive ATE = {naive_ate:.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Distribution of individual effects\n",
    "axes[2].hist(individual_effects, bins=20, density=True, alpha=0.7, edgecolor='black')\n",
    "axes[2].axvline(naive_ate, color='red', linestyle='--', lw=2, label=f'Naive ATE = {naive_ate:.2f}')\n",
    "axes[2].axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[2].set_xlabel('Individual Treatment Effect')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].set_title('True Distribution of Effects\\n(Heterogeneous!)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Treatment Effect Heterogeneity: Why DP Matters', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(f\"  - Naive ATE = {naive_ate:.2f} (average across all subgroups)\")\n",
    "print(f\"  - But true effects are: {list(true_effects.values())}\")\n",
    "print(f\"  - DP mixture models can discover these subgroups!\")\n",
    "print(f\"  - This enables personalized treatment decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Reference\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **DP(α, G₀)** | Distribution over distributions |\n",
    "| **Concentration α** | Controls number of clusters |\n",
    "| **Base measure G₀** | Prior for cluster locations |\n",
    "| **Stick-breaking** | Constructive definition via Beta draws |\n",
    "| **CRP** | Sequential clustering metaphor |\n",
    "| **Pólya urn** | Predictive distribution |\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "```\n",
    "Stick-breaking:\n",
    "  Vₖ ~ Beta(1, α)\n",
    "  πₖ = Vₖ ∏ⱼ₌₁ᵏ⁻¹ (1 - Vⱼ)\n",
    "  θₖ ~ G₀\n",
    "  G = Σₖ πₖ δ_θₖ\n",
    "\n",
    "CRP:\n",
    "  P(new table) = α / (n + α)\n",
    "  P(table k) = nₖ / (n + α)\n",
    "\n",
    "Expected clusters:\n",
    "  E[K] ≈ α log(1 + n/α)\n",
    "```\n",
    "\n",
    "### When to Use DP\n",
    "\n",
    "- Unknown number of clusters\n",
    "- Want data to determine complexity\n",
    "- Flexible nonparametric modeling\n",
    "- Discovering latent subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for future use\n",
    "\n",
    "def dp_summary(alpha, n):\n",
    "    \"\"\"Print summary for DP with given α and n observations.\"\"\"\n",
    "    expected_clusters = alpha * np.log(1 + n/alpha)\n",
    "    prob_new = alpha / (alpha + n)\n",
    "    \n",
    "    print(f\"DP(α={alpha}) with n={n} observations:\")\n",
    "    print(f\"  Expected clusters: {expected_clusters:.2f}\")\n",
    "    print(f\"  P(new cluster for n+1): {prob_new:.4f}\")\n",
    "    print(f\"  P(join existing): {1-prob_new:.4f}\")\n",
    "\n",
    "# Example\n",
    "dp_summary(alpha=2, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The Dirichlet Process is essential for:\n",
    "\n",
    "1. **Nonparametric clustering** — let data determine K\n",
    "2. **Flexible density estimation** — DP mixture models\n",
    "3. **Bayesian nonparametrics** — infinite-dimensional priors\n",
    "\n",
    "**For causal ML:**\n",
    "- Discover patient subgroups with heterogeneous treatment effects\n",
    "- Model complex outcome distributions\n",
    "- Handle unknown confounding structure\n",
    "\n",
    "**Key insight:** The concentration parameter α controls the \"rich get richer\" dynamics:\n",
    "- Small α → few dominant clusters\n",
    "- Large α → many small clusters\n",
    "\n",
    "**Next:** See `06_dp_mixture_models.ipynb` for practical applications to clustering and causal inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
